"""
Asystent HR dla pracodawcÃ³w zatrudniajÄ…cych osoby z niepeÅ‚nosprawnoÅ›ciami.
Wykorzystuje dokumenty PDF jako bazÄ™ wiedzy z wektorowÄ… bazÄ… danych w pamiÄ™ci.
"""

import os
import logging
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
import re

# LangChain imports (aktualne na 2024-06)
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores.faiss import FAISS
from langchain_community.document_loaders import PyPDFLoader
from langchain_core.documents import Document
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferWindowMemory
from langchain_core.prompts import PromptTemplate

# PDF processing
import fitz  # PyMuPDF
from sentence_transformers import SentenceTransformer

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class IntelligentPDFChunker:
    """
    Inteligentny chunker dla dokumentÃ³w PDF, ktÃ³ry respektuje strukturÄ™ dokumentu.
    """
    
    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
    def _detect_structure_markers(self, text: str) -> List[Tuple[int, str, int]]:
        """
        Wykrywa markery struktury w tekÅ›cie (nagÅ‚Ã³wki, punkty, etc.)
        Zwraca listÄ™ tupli (pozycja, typ, poziom)
        """
        markers = []
        
        # Wzorce dla rÃ³Å¼nych typÃ³w struktury
        patterns = [
            (r'^#{1,6}\s+(.+)', 'header', lambda m: len(m.group(0).split()[0])),
            (r'^##\s+(.+)', 'header', 2),
            (r'^###\s+(.+)', 'header', 3),
            (r'^####\s+(.+)', 'header', 4),
            (r'^#####\s+(.+)', 'header', 5),
            (r'^######\s+(.+)', 'header', 6),
            (r'^\d+\.\s+(.+)', 'numbered_list', 1),
            (r'^â€¢\s+(.+)', 'bullet_list', 1),
            (r'^-\s+(.+)', 'bullet_list', 1),
            (r'^\*\s+(.+)', 'bullet_list', 1),
            (r'^[A-ZÄ„Ä†Ä˜ÅÅƒÃ“ÅšÅ¹Å»][A-ZÄ„Ä†Ä˜ÅÅƒÃ“ÅšÅ¹Å»\s]+:$', 'section_title', 1),
            (r'^Krok\s+\d+', 'step', 1),
            (r'^RozdziaÅ‚\s+\d+', 'chapter', 1),
            (r'^CzÄ™Å›Ä‡\s+\d+', 'part', 1),
        ]
        
        lines = text.split('\n')
        for i, line in enumerate(lines):
            line = line.strip()
            if not line:
                continue
                
            for pattern, marker_type, level_func in patterns:
                match = re.match(pattern, line, re.IGNORECASE)
                if match:
                    level = level_func(match) if callable(level_func) else level_func
                    markers.append((i, marker_type, level))
                    break
        
        return markers
    
    def _extract_pdf_structure(self, pdf_path: str) -> List[Document]:
        """
        Ekstraktuje tekst z PDF z zachowaniem struktury dokumentu.
        """
        doc = fitz.open(pdf_path)
        documents = []
        
        for page_num in range(len(doc)):
            page = doc[page_num]
            
            # Pobierz tekst z metadanymi o czcionce
            blocks = page.get_text("dict")["blocks"]
            
            page_text = ""
            current_section = ""
            
            for block in blocks:
                if block.get("type") == 0:  # text block
                    for line in block["lines"]:
                        line_text = ""
                        max_size = 0
                        
                        for span in line["spans"]:
                            text = span.get("text", "").strip()
                            if text:
                                line_text += text + " "
                                max_size = max(max_size, span.get("size", 0))
                        
                        if line_text.strip():
                            # Wykryj nagÅ‚Ã³wki na podstawie rozmiaru czcionki
                            if max_size > 12:  # WiÄ™ksza czcionka = prawdopodobnie nagÅ‚Ã³wek
                                if current_section:
                                    # Zapisz poprzedniÄ… sekcjÄ™
                                    documents.append(Document(
                                        page_content=page_text.strip(),
                                        metadata={
                                            "source": pdf_path,
                                            "page": page_num + 1,
                                            "section": current_section,
                                            "type": "section"
                                        }
                                    ))
                                    page_text = ""
                                
                                current_section = line_text.strip()
                                page_text += f"\n## {line_text.strip()}\n"
                            else:
                                page_text += line_text.strip() + " "
            
            # Dodaj ostatniÄ… sekcjÄ™ ze strony
            if page_text.strip():
                documents.append(Document(
                    page_content=page_text.strip(),
                    metadata={
                        "source": pdf_path,
                        "page": page_num + 1,
                        "section": current_section or f"Strona {page_num + 1}",
                        "type": "section"
                    }
                ))
        
        doc.close()
        return documents
    
    def chunk_documents(self, documents: List[Document]) -> List[Document]:
        """
        Dzieli dokumenty na chunki z zachowaniem struktury.
        """
        chunked_docs = []
        
        for doc in documents:
            # JeÅ›li dokument jest maÅ‚y, zostaw go bez dzielenia
            if len(doc.page_content) <= self.chunk_size:
                chunked_docs.append(doc)
                continue
            
            # UÅ¼yj RecursiveCharacterTextSplitter z separatorami strukturalnymi
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=self.chunk_size,
                chunk_overlap=self.chunk_overlap,
                separators=[
                    "\n## ",      # NagÅ‚Ã³wki H2
                    "\n### ",     # NagÅ‚Ã³wki H3
                    "\n#### ",    # NagÅ‚Ã³wki H4
                    "\n\n",       # PodwÃ³jny enter
                    "\n",         # Pojedynczy enter
                    ". ",         # Koniec zdania
                    ", ",         # Przecinek
                    " ",          # Spacja
                    ""
                ]
            )
            
            texts = text_splitter.split_text(doc.page_content)
            
            for i, text in enumerate(texts):
                chunk_metadata = doc.metadata.copy()
                chunk_metadata["chunk_id"] = i
                chunk_metadata["total_chunks"] = len(texts)
                
                chunked_docs.append(Document(
                    page_content=text,
                    metadata=chunk_metadata
                ))
        
        return chunked_docs


class HRAssistant:
    """
    Asystent HR dla pracodawcÃ³w zatrudniajÄ…cych osoby z niepeÅ‚nosprawnoÅ›ciami.

    Wykorzystuje dokumenty PDF jako bazÄ™ wiedzy, przetwarza je na wektorowÄ… bazÄ™ danych (FAISS),
    a nastÄ™pnie umoÅ¼liwia zadawanie pytaÅ„ w jÄ™zyku polskim z konwersacyjnÄ… pamiÄ™ciÄ… kontekstu.
    Odpowiedzi generowane sÄ… przez model OpenAI GPT na podstawie treÅ›ci dokumentÃ³w.

    Parametry:
        openai_api_key (str): Klucz API do OpenAI.
        pdf_directory (str): ÅšcieÅ¼ka do katalogu z plikami PDF.
    """

    def _setup_qa_chain(self):
        """
        Tworzy i konfiguruje Å‚aÅ„cuch pytaÅ„ i odpowiedzi (ConversationalRetrievalChain) dla asystenta HR.
        ÅaÅ„cuch ten korzysta z modelu LLM, bazy wektorowej oraz pamiÄ™ci konwersacyjnej.
        Prompt jest zoptymalizowany pod polskie realia HR i bazuje wyÅ‚Ä…cznie na wiedzy z dokumentÃ³w PDF.

        Raises:
            ValueError: JeÅ›li baza wektorowa nie zostaÅ‚a zainicjalizowana.
        """
        if not self.vectorstore:
            raise ValueError("Baza wektorowa nie zostaÅ‚a zainicjalizowana.")

        prompt_template = (
            "JesteÅ› ekspertem HR specjalizujÄ…cym siÄ™ w zatrudnianiu osÃ³b z niepeÅ‚nosprawnoÅ›ciami w Polsce.\n"
            "Twoja wiedza opiera siÄ™ na oficjalnych dokumentach i poradnikach dla pracodawcÃ³w.\n\n"
            "Kontekst z dokumentÃ³w:\n{context}\n\n"
            "Historia rozmowy:\n{chat_history}\n\n"
            "Pytanie: {question}\n\n"
            "Instrukcje:\n"
            "1. Odpowiadaj w jÄ™zyku polskim\n"
            "2. Bazuj wyÅ‚Ä…cznie na informacjach z dostarczonych dokumentÃ³w\n"
            "3. JeÅ›li nie masz informacji w dokumentach, powiedz to wprost\n"
            "4. Podawaj konkretne, praktyczne porady\n"
            "5. OdwoÅ‚uj siÄ™ do konkretnych przepisÃ³w prawnych gdy to moÅ¼liwe\n"
            "6. BÄ…dÅº pomocny i profesjonalny\n"
            "7. Gdy to moÅ¼liwe, podaj ÅºrÃ³dÅ‚o informacji (nazwÄ™ dokumentu)\n\n"
            "OdpowiedÅº:"
        )
        custom_prompt = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "chat_history", "question"]
        )
        self.qa_chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=self.vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 5}
            ),
            memory=self.memory,
            combine_docs_chain_kwargs={"prompt": custom_prompt},
            return_source_documents=True,
            output_key="answer"
        )
    """
    Asystent HR dla pracodawcÃ³w zatrudniajÄ…cych osoby z niepeÅ‚nosprawnoÅ›ciami.
    """
    
    def __init__(self, openai_api_key: str, pdf_directory: str = "pdfs"):
        self.openai_api_key = openai_api_key
        self.pdf_directory = Path(pdf_directory)
        self._known_pdfs = set()
        self._pdf_mtimes = {}

        # Inicjalizuj komponenty
        self.embeddings = OpenAIEmbeddings(
            api_key=openai_api_key,
            model="text-embedding-3-small"
        )
        self.llm = ChatOpenAI(
            api_key=openai_api_key,
            model="gpt-4o-mini",
            temperature=0.3
        )
        self.chunker = IntelligentPDFChunker(
            chunk_size=1000,
            chunk_overlap=200
        )
        self.vectorstore = None
        self.qa_chain = None
        self.memory = ConversationBufferWindowMemory(
            k=5,
            memory_key="chat_history",
            return_messages=True,
            output_key="answer",
            input_key="question"
        )
        self._load_and_process_documents()
        self._setup_qa_chain()

    def _list_pdf_files(self):
        return list(self.pdf_directory.glob("*.pdf"))

    def _pdfs_changed(self) -> bool:
        """
        Sprawdza, czy pojawiÅ‚y siÄ™ nowe pliki PDF lub zmieniÅ‚y siÄ™ istniejÄ…ce.
        """
        changed = False
        current_pdfs = set()
        current_mtimes = {}
        for pdf in self._list_pdf_files():
            current_pdfs.add(pdf.name)
            mtime = pdf.stat().st_mtime
            current_mtimes[pdf.name] = mtime
            if (pdf.name not in self._pdf_mtimes) or (self._pdf_mtimes.get(pdf.name) != mtime):
                changed = True
        if self._known_pdfs != current_pdfs:
            changed = True
        if changed:
            self._known_pdfs = current_pdfs
            self._pdf_mtimes = current_mtimes
        return changed

    def _load_and_process_documents(self):
        """
        Åaduje i przetwarza dokumenty PDF.
        """
        logger.info("Åadowanie dokumentÃ³w PDF...")

        pdf_files = self._list_pdf_files()
        if not pdf_files:
            raise ValueError(f"Nie znaleziono plikÃ³w PDF w katalogu: {self.pdf_directory}")

        logger.info(f"Znaleziono {len(pdf_files)} plikÃ³w PDF")
        all_documents = []
        for pdf_file in pdf_files:
            logger.info(f"Przetwarzanie: {pdf_file.name}")
            documents = self.chunker._extract_pdf_structure(str(pdf_file))
            for doc in documents:
                doc.metadata["filename"] = pdf_file.name
                doc.metadata["file_stem"] = pdf_file.stem
            all_documents.extend(documents)
        logger.info(f"Wyekstraktowano {len(all_documents)} sekcji")
        chunked_documents = self.chunker.chunk_documents(all_documents)
        logger.info(f"Utworzono {len(chunked_documents)} chunkÃ³w")
        self.vectorstore = FAISS.from_documents(
            chunked_documents,
            self.embeddings
        )
        logger.info("Baza wektorowa zostaÅ‚a utworzona")

    def _reload_if_pdfs_changed(self):
        """
        PrzeÅ‚adowuje embeddingi jeÅ›li pojawiÅ‚y siÄ™ nowe/zmienione PDF-y.
        """
        if self._pdfs_changed():
            logger.info("Wykryto nowe lub zmienione pliki PDF. PrzeÅ‚adowujÄ™ bazÄ™ wiedzy...")
            self._load_and_process_documents()
            self._setup_qa_chain()

    def ask(self, question: str) -> Dict[str, Any]:
        """
        Zadaje pytanie asystentowi.
        """
        logger.info(f"Otrzymano pytanie: {question}")
        self._reload_if_pdfs_changed()
        try:
            result = self.qa_chain.invoke({
                "question": question,
                "chat_history": self.memory.chat_memory.messages
            })
            response = {
                "answer": result["answer"],
                "sources": [],
                "confidence": "medium"
            }
            for doc in result.get("source_documents", []):
                source_info = {
                    "filename": doc.metadata.get("filename", ""),
                    "page": doc.metadata.get("page", ""),
                    "section": doc.metadata.get("section", ""),
                    "snippet": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content
                }
                response["sources"].append(source_info)
            return response
        except Exception as e:
            logger.error(f"BÅ‚Ä…d podczas przetwarzania pytania: {e}")
            return {
                "answer": "Przepraszam, wystÄ…piÅ‚ bÅ‚Ä…d podczas przetwarzania Twojego pytania.",
                "sources": [],
                "confidence": "low",
                "error": str(e)
            }

    def get_stats(self) -> Dict[str, Any]:
        """
        Zwraca statystyki asystenta.
        """
        return {
            "total_documents": self.vectorstore.index.ntotal if self.vectorstore else 0,
            "pdf_files": len(self._list_pdf_files()),
            "memory_messages": len(self.memory.chat_memory.messages),
            "model": "gpt-4o-mini",
            "embedding_model": "text-embedding-3-small"
        }
    
    def clear_memory(self):
        """
        CzyÅ›ci pamiÄ™Ä‡ konwersacji.
        """
        self.memory.clear()
        logger.info("PamiÄ™Ä‡ konwersacji zostaÅ‚a wyczyszczona")


def print_unique_sources(sources: list):
    """
    Wypisuje unikalne ÅºrÃ³dÅ‚a na podstawie filename, page, section.
    """
    unique_sources = []
    seen = set()
    for source in sources:
        key = (source['filename'], source['page'], source['section'])
        if key not in seen:
            seen.add(key)
            unique_sources.append(source)
    for i, source in enumerate(unique_sources, 1):
        print(f"{i}. {source['filename']} (str. {source['page']}) - {source['section']}")

def handle_command(command: str, assistant: HRAssistant) -> bool:
    """
    ObsÅ‚uguje polecenia specjalne. Zwraca True jeÅ›li naleÅ¼y kontynuowaÄ‡ pÄ™tlÄ™.
    """
    cmd = command.lower()
    if cmd in ['quit', 'exit', 'q']:
        return False
    if cmd == 'stats':
        print(f"Statystyki: {assistant.get_stats()}")
        return True
    if cmd == 'clear':
        assistant.clear_memory()
        print("PamiÄ™Ä‡ konwersacji zostaÅ‚a wyczyszczona")
        return True
    return None

def main():
    """
    PrzykÅ‚ad uÅ¼ycia asystenta HR.
    """
    # SprawdÅº czy ustawiono klucz API
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("Ustaw zmiennÄ… Å›rodowiskowÄ… OPENAI_API_KEY")
    
    # UtwÃ³rz asystenta
    assistant = HRAssistant(
        openai_api_key=api_key,
        pdf_directory="pdfs"
    )
    
    # PrzykÅ‚adowe pytania
    test_questions = [
        "Jakie sÄ… uprawnienia pracownika z niepeÅ‚nosprawnoÅ›ciÄ…?",
        "Jak przeprowadziÄ‡ rekrutacjÄ™ osoby z niepeÅ‚nosprawnoÅ›ciÄ…?",
        "Jakie wsparcie moÅ¼e otrzymaÄ‡ pracodawca zatrudniajÄ…cy osoby z niepeÅ‚nosprawnoÅ›ciami?",
        "Czy osoba z orzeczeniem o caÅ‚kowitej niezdolnoÅ›ci do pracy moÅ¼e byÄ‡ zatrudniona?"
    ]
    
    print("=== Asystent HR - Zatrudnianie osÃ³b z niepeÅ‚nosprawnoÅ›ciami ===\n")
    print(f"Statystyki: {assistant.get_stats()}\n")
    
    # Interaktywny tryb
    while True:
        try:
            question = input("\nTwoje pytanie (lub 'quit' aby zakoÅ„czyÄ‡): ")
            if not question.strip():
                continue

            cmd_result = handle_command(question, assistant)
            if cmd_result is False:
                break
            if cmd_result is True:
                continue

            # Uzyskaj odpowiedÅº
            response = assistant.ask(question)
            print(f"\nğŸ“ OdpowiedÅº:")
            print(response["answer"])

            if "error" in response:
                print(f"\nâš ï¸  BÅ‚Ä…d: {response['error']}")

        except KeyboardInterrupt:
            print("\n\nDo widzenia!")
            break
        except Exception as e:
            print(f"\nâŒ BÅ‚Ä…d: {e}")


if __name__ == "__main__":
    main()


# ===============================
# Instrukcja wdroÅ¼enia moduÅ‚u HRAssistant
# ===============================
#
# 1. Ustaw zmiennÄ… Å›rodowiskowÄ… z kluczem OpenAI:
#    export OPENAI_API_KEY="twoj_klucz_openai"
#
# 2. UmieÅ›Ä‡ pliki PDF w katalogu "pdfs" (lub wskaÅ¼ inny katalog w parametrze pdf_directory).
#
# 3. Zainstaluj wymagane biblioteki:
#    pip install -r requirements.txt
#
# 4. Uruchom moduÅ‚:
#    python hr_assistant.py
#
# 5. MoÅ¼esz zintegrowaÄ‡ klasÄ™ HRAssistant w swoim projekcie:
#    from hr_assistant import HRAssistant
#    assistant = HRAssistant(openai_api_key="...", pdf_directory="pdfs")
#    odpowiedz = assistant.ask("Twoje pytanie")
#
# 6. SzczegÃ³Å‚y i przykÅ‚ady znajdziesz w README.md oraz EXAMPLES.md.
