"""
Asystent HR dla pracodawcÃ³w zatrudniajÄ…cych osoby z niepeÅ‚nosprawnoÅ›ciami.
Wykorzystuje dokumenty PDF oraz treÅ›ci ze stron internetowych jako bazÄ™ wiedzy 
z wektorowÄ… bazÄ… danych w pamiÄ™ci.
"""

import os
import logging
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
import re

# LangChain imports (aktualne na 2024-06)
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores.faiss import FAISS
from langchain_community.document_loaders import PyPDFLoader
from langchain_core.documents import Document
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferWindowMemory
from langchain_core.prompts import PromptTemplate

# PDF processing
import fitz  # PyMuPDF
from sentence_transformers import SentenceTransformer
import pandas as pd

# Web scraping for URLs
import requests
from bs4 import BeautifulSoup
from datetime import datetime

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class IntelligentPDFChunker:
    """
    Inteligentny chunker dla dokumentÃ³w PDF, ktÃ³ry respektuje strukturÄ™ dokumentu.
    """
    
    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
    def _detect_structure_markers(self, text: str) -> List[Tuple[int, str, int]]:
        """
        Wykrywa markery struktury w tekÅ›cie (nagÅ‚Ã³wki, punkty, etc.)
        Zwraca listÄ™ tupli (pozycja, typ, poziom)
        """
        markers = []
        
        # Wzorce dla rÃ³Å¼nych typÃ³w struktury
        patterns = [
            (r'^#{1,6}\s+(.+)', 'header', lambda m: len(m.group(0).split()[0])),
            (r'^##\s+(.+)', 'header', 2),
            (r'^###\s+(.+)', 'header', 3),
            (r'^####\s+(.+)', 'header', 4),
            (r'^#####\s+(.+)', 'header', 5),
            (r'^######\s+(.+)', 'header', 6),
            (r'^\d+\.\s+(.+)', 'numbered_list', 1),
            (r'^â€¢\s+(.+)', 'bullet_list', 1),
            (r'^-\s+(.+)', 'bullet_list', 1),
            (r'^\*\s+(.+)', 'bullet_list', 1),
            (r'^[A-ZÄ„Ä†Ä˜ÅÅƒÃ“ÅšÅ¹Å»][A-ZÄ„Ä†Ä˜ÅÅƒÃ“ÅšÅ¹Å»\s]+:$', 'section_title', 1),
            (r'^Krok\s+\d+', 'step', 1),
            (r'^RozdziaÅ‚\s+\d+', 'chapter', 1),
            (r'^CzÄ™Å›Ä‡\s+\d+', 'part', 1),
        ]
        
        lines = text.split('\n')
        for i, line in enumerate(lines):
            line = line.strip()
            if not line:
                continue
                
            for pattern, marker_type, level_func in patterns:
                match = re.match(pattern, line, re.IGNORECASE)
                if match:
                    level = level_func(match) if callable(level_func) else level_func
                    markers.append((i, marker_type, level))
                    break
        
        return markers
    
    def _extract_pdf_structure(self, pdf_path: str) -> List[Document]:
        """
        Ekstraktuje tekst z PDF z zachowaniem struktury dokumentu.
        """
        doc = fitz.open(pdf_path)
        documents = []
        
        for page_num in range(len(doc)):
            page = doc[page_num]
            
            # Pobierz tekst z metadanymi o czcionce
            blocks = page.get_text("dict")["blocks"]
            
            page_text = ""
            current_section = ""
            
            for block in blocks:
                if block.get("type") == 0:  # text block
                    for line in block["lines"]:
                        line_text = ""
                        max_size = 0
                        
                        for span in line["spans"]:
                            text = span.get("text", "").strip()
                            if text:
                                line_text += text + " "
                                max_size = max(max_size, span.get("size", 0))
                        
                        if line_text.strip():
                            # Wykryj nagÅ‚Ã³wki na podstawie rozmiaru czcionki
                            if max_size > 12:  # WiÄ™ksza czcionka = prawdopodobnie nagÅ‚Ã³wek
                                if current_section:
                                    # Zapisz poprzedniÄ… sekcjÄ™
                                    documents.append(Document(
                                        page_content=page_text.strip(),
                                        metadata={
                                            "source": pdf_path,
                                            "page": page_num + 1,
                                            "section": current_section,
                                            "type": "section"
                                        }
                                    ))
                                    page_text = ""
                                
                                current_section = line_text.strip()
                                page_text += f"\n## {line_text.strip()}\n"
                            else:
                                page_text += line_text.strip() + " "
            
            # Dodaj ostatniÄ… sekcjÄ™ ze strony
            if page_text.strip():
                documents.append(Document(
                    page_content=page_text.strip(),
                    metadata={
                        "source": pdf_path,
                        "page": page_num + 1,
                        "section": current_section or f"Strona {page_num + 1}",
                        "type": "section"
                    }
                ))
        
        doc.close()
        return documents
    
    def chunk_documents(self, documents: List[Document]) -> List[Document]:
        """
        Dzieli dokumenty na chunki z zachowaniem struktury.
        """
        chunked_docs = []
        
        for doc in documents:
            # JeÅ›li dokument jest maÅ‚y, zostaw go bez dzielenia
            if len(doc.page_content) <= self.chunk_size:
                chunked_docs.append(doc)
                continue
            
            # UÅ¼yj RecursiveCharacterTextSplitter z separatorami strukturalnymi
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=self.chunk_size,
                chunk_overlap=self.chunk_overlap,
                separators=[
                    "\n## ",      # NagÅ‚Ã³wki H2
                    "\n### ",     # NagÅ‚Ã³wki H3
                    "\n#### ",    # NagÅ‚Ã³wki H4
                    "\n\n",       # PodwÃ³jny enter
                    "\n",         # Pojedynczy enter
                    ". ",         # Koniec zdania
                    ", ",         # Przecinek
                    " ",          # Spacja
                    ""
                ]
            )
            
            texts = text_splitter.split_text(doc.page_content)
            
            for i, text in enumerate(texts):
                chunk_metadata = doc.metadata.copy()
                chunk_metadata["chunk_id"] = i
                chunk_metadata["total_chunks"] = len(texts)
                
                chunked_docs.append(Document(
                    page_content=text,
                    metadata=chunk_metadata
                ))
        
        return chunked_docs


class HRAssistant:
    """
    Asystent HR dla pracodawcÃ³w zatrudniajÄ…cych osoby z niepeÅ‚nosprawnoÅ›ciami.

    Wykorzystuje dokumenty PDF oraz treÅ›ci ze stron internetowych jako bazÄ™ wiedzy, 
    przetwarza je na wektorowÄ… bazÄ™ danych (FAISS), a nastÄ™pnie umoÅ¼liwia zadawanie 
    pytaÅ„ w jÄ™zyku polskim z konwersacyjnÄ… pamiÄ™ciÄ… kontekstu.
    Odpowiedzi generowane sÄ… przez model OpenAI GPT na podstawie treÅ›ci dokumentÃ³w.

    Parametry:
        openai_api_key (str): Klucz API do OpenAI.
        pdf_directory (str): ÅšcieÅ¼ka do katalogu z plikami PDF.
        urls_file (str): ÅšcieÅ¼ka do pliku z listÄ… URLs do zaÅ‚adowania.
    """

    def _setup_qa_chain(self):
        """
        Tworzy i konfiguruje Å‚aÅ„cuch pytaÅ„ i odpowiedzi (ConversationalRetrievalChain) dla asystenta HR.
        ÅaÅ„cuch ten korzysta z modelu LLM, bazy wektorowej oraz pamiÄ™ci konwersacyjnej.
        Prompt jest zoptymalizowany pod polskie realia HR i bazuje wyÅ‚Ä…cznie na wiedzy z dokumentÃ³w PDF.

        Raises:
            ValueError: JeÅ›li baza wektorowa nie zostaÅ‚a zainicjalizowana.
        """
        if not self.vectorstore:
            raise ValueError("Baza wektorowa nie zostaÅ‚a zainicjalizowana.")

        prompt_template = (
            "JesteÅ› ekspertem HR specjalizujÄ…cym siÄ™ w zatrudnianiu osÃ³b z niepeÅ‚nosprawnoÅ›ciami w Polsce.\n"
            "Twoja wiedza opiera siÄ™ na oficjalnych dokumentach, poradnikach dla pracodawcÃ³w oraz aktualnych informacjach ze stron PFRON.\n\n"
            "Kontekst z dokumentÃ³w:\n{context}\n\n"
            "Historia rozmowy:\n{chat_history}\n\n"
            "Pytanie: {question}\n\n"
            "Instrukcje:\n"
            "1. Odpowiadaj w jÄ™zyku polskim\n"
            "2. Bazuj wyÅ‚Ä…cznie na informacjach z dostarczonych dokumentÃ³w i stron internetowych\n"
            "3. JeÅ›li nie masz informacji w dokumentach, powiedz to wprost\n"
            "4. Podawaj konkretne, praktyczne porady\n"
            "5. OdwoÅ‚uj siÄ™ do konkretnych przepisÃ³w prawnych gdy to moÅ¼liwe\n"
            "6. BÄ…dÅº pomocny i profesjonalny\n"
            "7. Gdy to moÅ¼liwe, podaj ÅºrÃ³dÅ‚o informacji (nazwÄ™ dokumentu lub stronÄ™ internetowÄ…)\n"
            "8. Dla informacji ze stron internetowych podaj datÄ™ dostÄ™pu jeÅ›li jest dostÄ™pna\n\n"
            "OdpowiedÅº:"
        )
        custom_prompt = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "chat_history", "question"]
        )
        self.qa_chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=self.vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 5}
            ),
            memory=self.memory,
            combine_docs_chain_kwargs={"prompt": custom_prompt},
            return_source_documents=True,
            output_key="answer"
        )
    """
    Asystent HR dla pracodawcÃ³w zatrudniajÄ…cych osoby z niepeÅ‚nosprawnoÅ›ciami.
    """
    
    def __init__(self, openai_api_key: str, pdf_directory: str = "pdfs", urls_file: str = "urls.txt"):
        self.openai_api_key = openai_api_key
        self.pdf_directory = Path(pdf_directory)
        self.urls_file = urls_file
        self._known_pdfs = set()
        self._pdf_mtimes = {}
        
        # ZaÅ‚aduj bibliografiÄ™ z pliku CSV
        self.bibliography = self._load_bibliography()

        # Inicjalizuj komponenty
        self.embeddings = OpenAIEmbeddings(
            api_key=openai_api_key,
            model="text-embedding-3-small"
        )
        self.llm = ChatOpenAI(
            api_key=openai_api_key,
            model="gpt-4o-mini",
            temperature=0.3
        )
        self.chunker = IntelligentPDFChunker(
            chunk_size=1000,
            chunk_overlap=200
        )
        self.vectorstore = None
        self.qa_chain = None
        self.memory = ConversationBufferWindowMemory(
            k=5,
            memory_key="chat_history",
            return_messages=True,
            output_key="answer",
            input_key="question"
        )
        self._load_and_process_documents()
        self._setup_qa_chain()

    def _load_bibliography(self) -> Dict[str, str]:
        """
        Åaduje bibliografiÄ™ z pliku bibliografia.csv.
        Zwraca sÅ‚ownik mapujÄ…cy nazwÄ™ pliku na peÅ‚ny opis bibliograficzny.
        """
        bibliography_path = Path("bibliografia.csv")
        if not bibliography_path.exists():
            logger.warning("Plik bibliografia.csv nie istnieje. BÄ™dÄ… uÅ¼ywane nazwy plikÃ³w.")
            return {}
        
        try:
            df = pd.read_csv(bibliography_path, delimiter=';')
            bibliography = {}
            for _, row in df.iterrows():
                filename = row['filename']
                opis = row['opis']
                bibliography[filename] = opis
            logger.info(f"ZaÅ‚adowano bibliografiÄ™ dla {len(bibliography)} dokumentÃ³w")
            return bibliography
        except Exception as e:
            logger.error(f"BÅ‚Ä…d podczas Å‚adowania bibliografii: {e}")
            return {}

    def _list_pdf_files(self):
        return list(self.pdf_directory.glob("*.pdf"))

    def _load_url_documents(self) -> List[Document]:
        """
        Pobiera i przetwarza treÅ›ci ze stron internetowych z pliku URLs.
        """
        if not os.path.exists(self.urls_file):
            logger.warning(f"Plik '{self.urls_file}' nie zostaÅ‚ znaleziony. Pomijam Å‚adowanie URLs.")
            return []
        
        try:
            with open(self.urls_file, 'r', encoding='utf-8') as f:
                urls = [line.strip() for line in f if line.strip() and not line.startswith('#')]
        except Exception as e:
            logger.error(f"BÅ‚Ä…d podczas odczytu pliku {self.urls_file}: {e}")
            return []
        
        if not urls:
            logger.info("Brak URLs do przetworzenia.")
            return []

        logger.info(f"Znaleziono {len(urls)} adresÃ³w URL do przetworzenia.")
        url_documents = []
        
        for url in urls:
            try:
                logger.info(f"Pobieranie: {url}")
                response = requests.get(
                    url, 
                    timeout=15, 
                    headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
                )
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                title = soup.find('title').get_text().strip() if soup.find('title') else 'Brak tytuÅ‚u'
                content = self._extract_url_content(soup)
                
                if content and len(content.strip()) > 50:  # Minimum content threshold
                    metadata = {
                        'source': url,
                        'title': title,
                        'type': 'website',
                        'added_date': datetime.now().strftime("%Y-%m-%d"),
                        'bibliography': f"PFRON, {title}, dostÄ™p: {datetime.now().strftime('%d.%m.%Y')}, {url}"
                    }
                    
                    # Check if contains financial data
                    if re.search(r'\d+(?:[.,]\d+)?\s*(?:zÅ‚|PLN|zÅ‚ot)', content, re.IGNORECASE):
                        metadata["contains_financial_data"] = True
                    
                    url_documents.append(Document(
                        page_content=content, 
                        metadata=metadata
                    ))
                    logger.info(f"Pobrano treÅ›Ä‡: {len(content)} znakÃ³w")
                else:
                    logger.warning(f"Brak wystarczajÄ…cej treÅ›ci dla: {url}")
                    
            except requests.RequestException as e:
                logger.error(f"BÅ‚Ä…d podczas pobierania {url}: {e}")
            except Exception as e:
                logger.error(f"Nieoczekiwany bÅ‚Ä…d podczas przetwarzania {url}: {e}")
        
        logger.info(f"PomyÅ›lnie pobrano {len(url_documents)} dokumentÃ³w z URLs")
        return url_documents

    def _extract_url_content(self, soup: BeautifulSoup) -> str:
        """
        WyodrÄ™bnia gÅ‚Ã³wnÄ… treÅ›Ä‡ ze strony internetowej.
        PrÃ³buje rÃ³Å¼nych selektorÃ³w CSS aby znaleÅºÄ‡ gÅ‚Ã³wnÄ… treÅ›Ä‡.
        """
        # Selektory specyficzne dla stron PFRON
        selectors = [
            '.csc-textpic-text.article-content',  # GÅ‚Ã³wny selektor dla PFRON
            '.frame.default',
            '.csc-default', 
            'article', 
            'main', 
            '.content', 
            '#content',
            '.main-content',
            '.page-content'
        ]
        
        for selector in selectors:
            elements = soup.select(selector)
            if elements:
                # PoÅ‚Ä…cz tekst z wszystkich znalezionych elementÃ³w
                content_parts = []
                for element in elements:
                    text = element.get_text(separator='\n', strip=True)
                    if text and len(text) > 20:  # Ignore very short content
                        content_parts.append(text)
                
                if content_parts:
                    return '\n\n'.join(content_parts)
        
        # Fallback - pobierz tekst z body
        if soup.body:
            # Remove script and style elements
            for script in soup.body(["script", "style", "nav", "footer", "header"]):
                script.decompose()
            return soup.body.get_text(separator='\n', strip=True)
        
        return ""

    def _pdfs_changed(self) -> bool:
        """
        Sprawdza, czy pojawiÅ‚y siÄ™ nowe pliki PDF lub zmieniÅ‚y siÄ™ istniejÄ…ce.
        """
        changed = False
        current_pdfs = set()
        current_mtimes = {}
        for pdf in self._list_pdf_files():
            current_pdfs.add(pdf.name)
            mtime = pdf.stat().st_mtime
            current_mtimes[pdf.name] = mtime
            if (pdf.name not in self._pdf_mtimes) or (self._pdf_mtimes.get(pdf.name) != mtime):
                changed = True
        if self._known_pdfs != current_pdfs:
            changed = True
        if changed:
            self._known_pdfs = current_pdfs
            self._pdf_mtimes = current_mtimes
        return changed

    def _load_and_process_documents(self):
        """
        Åaduje i przetwarza dokumenty PDF oraz treÅ›ci z URLs.
        """
        logger.info("Åadowanie dokumentÃ³w PDF i treÅ›ci z URLs...")

        # Åadowanie PDF
        pdf_files = self._list_pdf_files()
        if not pdf_files:
            logger.warning(f"Nie znaleziono plikÃ³w PDF w katalogu: {self.pdf_directory}")
        
        all_documents = []
        
        # Przetwarzanie plikÃ³w PDF
        if pdf_files:
            logger.info(f"Znaleziono {len(pdf_files)} plikÃ³w PDF")
            for pdf_file in pdf_files:
                logger.info(f"Przetwarzanie PDF: {pdf_file.name}")
                documents = self.chunker._extract_pdf_structure(str(pdf_file))
                for doc in documents:
                    doc.metadata["filename"] = pdf_file.name
                    doc.metadata["file_stem"] = pdf_file.stem
                    # Dodaj peÅ‚ny opis bibliograficzny jeÅ›li dostÄ™pny
                    if pdf_file.name in self.bibliography:
                        doc.metadata["bibliography"] = self.bibliography[pdf_file.name]
                    else:
                        doc.metadata["bibliography"] = pdf_file.stem  # fallback na nazwÄ™ pliku
                all_documents.extend(documents)
            logger.info(f"Wyekstraktowano {len(all_documents)} sekcji z PDF")
        
        # Åadowanie treÅ›ci z URLs
        url_documents = self._load_url_documents()
        all_documents.extend(url_documents)
        
        if not all_documents:
            raise ValueError("Nie znaleziono Å¼adnych dokumentÃ³w do przetworzenia (ani PDF ani URLs)")
        
        logger.info(f"ÅÄ…cznie dokumentÃ³w do przetworzenia: {len(all_documents)} (PDF: {len(all_documents) - len(url_documents)}, URLs: {len(url_documents)})")
        
        chunked_documents = self.chunker.chunk_documents(all_documents)
        logger.info(f"Utworzono {len(chunked_documents)} chunkÃ³w")
        
        self.vectorstore = FAISS.from_documents(
            chunked_documents,
            self.embeddings
        )
        logger.info("Baza wektorowa zostaÅ‚a utworzona")
        
        # Zaktualizuj zmienne Å›ledzÄ…ce pliki PDF po pomyÅ›lnym zaÅ‚adowaniu
        self._known_pdfs = set()
        self._pdf_mtimes = {}
        for pdf_file in pdf_files:
            self._known_pdfs.add(pdf_file.name)
            self._pdf_mtimes[pdf_file.name] = pdf_file.stat().st_mtime

    def _reload_if_pdfs_changed(self):
        """
        PrzeÅ‚adowuje embeddingi jeÅ›li pojawiÅ‚y siÄ™ nowe/zmienione PDF-y.
        """
        if self._pdfs_changed():
            logger.info("Wykryto nowe lub zmienione pliki PDF. PrzeÅ‚adowujÄ™ bazÄ™ wiedzy...")
            self._load_and_process_documents()
            self._setup_qa_chain()

    def ask(self, question: str) -> Dict[str, Any]:
        """
        Zadaje pytanie asystentowi.
        """
        logger.info(f"Otrzymano pytanie: {question}")
        # UsuniÄ™te automatyczne sprawdzanie zmian - baza Å‚adowana tylko przy starcie
        try:
            result = self.qa_chain.invoke({
                "question": question,
                "chat_history": self.memory.chat_memory.messages
            })
            response = {
                "answer": result["answer"],
                "sources": [],
                "confidence": "medium"
            }
            for doc in result.get("source_documents", []):
                source_info = {
                    "filename": doc.metadata.get("filename", ""),
                    "bibliography": doc.metadata.get("bibliography", doc.metadata.get("filename", "")),
                    "page": doc.metadata.get("page", ""),
                    "section": doc.metadata.get("section", ""),
                    "snippet": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content
                }
                response["sources"].append(source_info)
            return response
        except Exception as e:
            logger.error(f"BÅ‚Ä…d podczas przetwarzania pytania: {e}")
            return {
                "answer": "Przepraszam, wystÄ…piÅ‚ bÅ‚Ä…d podczas przetwarzania Twojego pytania.",
                "sources": [],
                "confidence": "low",
                "error": str(e)
            }

    def reload_knowledge_base(self):
        """
        RÄ™cznie przeÅ‚adowuje bazÄ™ wiedzy (sprawdza zmiany w plikach PDF).
        """
        logger.info("RÄ™czne przeÅ‚adowanie bazy wiedzy...")
        if self._pdfs_changed():
            logger.info("Wykryto zmiany w plikach PDF. PrzeÅ‚adowujÄ™ bazÄ™ wiedzy...")
            self._load_and_process_documents()
            self._setup_qa_chain()
            return True
        else:
            logger.info("Brak zmian w plikach PDF.")
            return False

    def get_stats(self) -> Dict[str, Any]:
        """
        Zwraca statystyki asystenta.
        """
        # Policz URLs z pliku
        url_count = 0
        if os.path.exists(self.urls_file):
            try:
                with open(self.urls_file, 'r', encoding='utf-8') as f:
                    url_count = len([line.strip() for line in f if line.strip() and not line.startswith('#')])
            except:
                pass
        
        return {
            "total_documents": self.vectorstore.index.ntotal if self.vectorstore else 0,
            "pdf_files": len(self._list_pdf_files()),
            "url_sources": url_count,
            "memory_messages": len(self.memory.chat_memory.messages),
            "model": "gpt-4o-mini",
            "embedding_model": "text-embedding-3-small"
        }
    
    def clear_memory(self):
        """
        CzyÅ›ci pamiÄ™Ä‡ konwersacji.
        """
        self.memory.clear()
        logger.info("PamiÄ™Ä‡ konwersacji zostaÅ‚a wyczyszczona")


def print_unique_sources(sources: list):
    """
    Wypisuje unikalne ÅºrÃ³dÅ‚a na podstawie filename, page, section.
    """
    unique_sources = []
    seen = set()
    for source in sources:
        key = (source['filename'], source['page'], source['section'])
        if key not in seen:
            seen.add(key)
            unique_sources.append(source)
    for i, source in enumerate(unique_sources, 1):
        print(f"{i}. {source['filename']} (str. {source['page']}) - {source['section']}")

def handle_command(command: str, assistant: HRAssistant) -> bool:
    """
    ObsÅ‚uguje polecenia specjalne. Zwraca True jeÅ›li naleÅ¼y kontynuowaÄ‡ pÄ™tlÄ™.
    """
    cmd = command.lower()
    if cmd in ['quit', 'exit', 'q']:
        return False
    if cmd == 'stats':
        print(f"Statystyki: {assistant.get_stats()}")
        return True
    if cmd == 'clear':
        assistant.clear_memory()
        print("PamiÄ™Ä‡ konwersacji zostaÅ‚a wyczyszczona")
        return True
    return None

def main():
    """
    PrzykÅ‚ad uÅ¼ycia asystenta HR.
    """
    # SprawdÅº czy ustawiono klucz API
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("Ustaw zmiennÄ… Å›rodowiskowÄ… OPENAI_API_KEY")
    
    # UtwÃ³rz asystenta
    assistant = HRAssistant(
        openai_api_key=api_key,
        pdf_directory="pdfs"
    )
    
    # PrzykÅ‚adowe pytania
    test_questions = [
        "Jakie sÄ… uprawnienia pracownika z niepeÅ‚nosprawnoÅ›ciÄ…?",
        "Jak przeprowadziÄ‡ rekrutacjÄ™ osoby z niepeÅ‚nosprawnoÅ›ciÄ…?",
        "Jakie wsparcie moÅ¼e otrzymaÄ‡ pracodawca zatrudniajÄ…cy osoby z niepeÅ‚nosprawnoÅ›ciami?",
        "Czy osoba z orzeczeniem o caÅ‚kowitej niezdolnoÅ›ci do pracy moÅ¼e byÄ‡ zatrudniona?"
    ]
    
    print("=== Asystent HR - Zatrudnianie osÃ³b z niepeÅ‚nosprawnoÅ›ciami ===\n")
    print(f"Statystyki: {assistant.get_stats()}\n")
    
    # Interaktywny tryb
    while True:
        try:
            question = input("\nTwoje pytanie (lub 'quit' aby zakoÅ„czyÄ‡): ")
            if not question.strip():
                continue

            cmd_result = handle_command(question, assistant)
            if cmd_result is False:
                break
            if cmd_result is True:
                continue

            # Uzyskaj odpowiedÅº
            response = assistant.ask(question)
            print(f"\nğŸ“ OdpowiedÅº:")
            print(response["answer"])

            if "error" in response:
                print(f"\nâš ï¸  BÅ‚Ä…d: {response['error']}")

        except KeyboardInterrupt:
            print("\n\nDo widzenia!")
            break
        except Exception as e:
            print(f"\nâŒ BÅ‚Ä…d: {e}")


if __name__ == "__main__":
    main()


# ===============================
# Instrukcja wdroÅ¼enia moduÅ‚u HRAssistant
# ===============================
#
# 1. Ustaw zmiennÄ… Å›rodowiskowÄ… z kluczem OpenAI:
#    export OPENAI_API_KEY="twoj_klucz_openai"
#
# 2. UmieÅ›Ä‡ pliki PDF w katalogu "pdfs" (lub wskaÅº inny katalog w parametrze pdf_directory).
#
# 3. Zainstaluj wymagane biblioteki:
#    pip install -r requirements.txt
#
# 4. Uruchom moduÅ‚:
#    python hr_assistant.py
#
# 5. MoÅ¼esz zintegrowaÄ‡ klasÄ™ HRAssistant w swoim projekcie:
#    from hr_assistant import HRAssistant
#    assistant = HRAssistant(openai_api_key="...", pdf_directory="pdfs")
#    odpowiedz = assistant.ask("Twoje pytanie")
#
# 6. SzczegÃ³Å‚y i przykÅ‚ady znajdziesz w README.md oraz EXAMPLES.md.
# 6. SzczegÃ³Å‚y i przykÅ‚ady znajdziesz w README.md oraz EXAMPLES.md.
