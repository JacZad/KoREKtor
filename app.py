import gradio as gr
import pandas as pd
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from pydantic import BaseModel, Field, field_validator
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader
from langchain.output_parsers import PydanticOutputParser
from docx import Document
from datetime import datetime
import os
import tempfile

# Import modu≈Çu hr_assistant
try:
    from hr_assistant import HRAssistant
    HR_ASSISTANT_AVAILABLE = True
except ImportError:
    HR_ASSISTANT_AVAILABLE = False
    print("Uwaga: Modu≈Ç hr_assistant nie jest dostƒôpny.")

# Globalna instancja asystenta HR
hr_assistant = None

def initialize_hr_assistant():
    """Inicjalizuje asystenta HR"""
    global hr_assistant, HR_ASSISTANT_AVAILABLE
    
    if not HR_ASSISTANT_AVAILABLE:
        return False
    
    try:
        openai_api_key = os.getenv("OPENAI_API_KEY")
        if not openai_api_key:
            print("Uwaga: Brak klucza OPENAI_API_KEY. Ekspert HR bƒôdzie wy≈ÇƒÖczony.")
            HR_ASSISTANT_AVAILABLE = False
            return False
        
        hr_assistant = HRAssistant(
            openai_api_key=openai_api_key,
            pdf_directory="pdfs"  # Dostosuj ≈õcie≈ºkƒô do swoich potrzeb
        )
        print("‚úÖ Asystent HR zosta≈Ç zainicjalizowany pomy≈õlnie.")
        return True
        
    except Exception as e:
        print(f"‚ùå B≈ÇƒÖd podczas inicjalizacji asystenta HR: {e}")
        HR_ASSISTANT_AVAILABLE = False
        return False

# Inicjalizuj asystenta przy starcie
initialize_hr_assistant()

# --- MODELE DANYCH (PYDANTIC) ---
# DefiniujƒÖ strukturƒô danych u≈ºywanƒÖ do parsowania odpowiedzi z LLM
# oraz do generowania finalnego wyniku JSON.

class QuestionAnswer(BaseModel):
    """
    Reprezentuje pojedynczƒÖ odpowied≈∫ na pytanie analityczne.
    Ten model jest u≈ºywany przez parser LangChain do strukturyzacji odpowiedzi LLM.
    """
    question_number: int = Field(..., description="Numer pytania z wewnƒôtrznej matrycy.")
    answer: str = Field(..., description="Odpowied≈∫ 'TAK' lub 'NIE'.")
    citation: str = Field(..., description="Cytat z analizowanego tekstu, na podstawie kt√≥rego udzielono odpowiedzi.")

    @field_validator("answer")
    def validate_answer(cls, v):
        """Walidator sprawdzajƒÖcy, czy odpowied≈∫ to 'TAK' lub 'NIE'."""
        if v not in {"TAK", "NIE"}:
            raise ValueError("Odpowied≈∫ musi byƒá TAK lub NIE")
        return v

class JobAdAnalysis(BaseModel):
    """
    Reprezentuje pe≈ÇnƒÖ analizƒô og≈Çoszenia, zawierajƒÖcƒÖ listƒô odpowiedzi.
    Ten model jest u≈ºywany przez parser LangChain do strukturyzacji odpowiedzi LLM.
    """
    answers: list[QuestionAnswer]

parser = PydanticOutputParser(pydantic_object=JobAdAnalysis)

PROMPT_TEMPLATE_TEXT = """Przeanalizuj poni≈ºsze og≈Çoszenie o pracƒô pod kƒÖtem dostƒôpno≈õci dla os√≥b z niepe≈Çnosprawno≈õciami.

Og≈Çoszenie:
{job_ad}

Odpowiedz na nastƒôpujƒÖce pytania:
{questions}

Format odpowiedzi powinien byƒá w nastƒôpujƒÖcej strukturze JSON:
{{
  "answers": [
    {{
      "question_number": 1,
      "answer": "TAK/NIE",
      "citation": "dok≈Çadny cytat z tekstu"
    }}
  ]
}}
"""

# Wczytanie matrycy danych
matryca_df = pd.read_csv('matryca.csv', header=None,
                         names=['area', 'prompt', 'true', 'false', 'more', 'hint'])

def prepare_questions(df):
    questions_text = ""
    for index, row in df.iterrows():
        question_number = index + 1
        questions_text += f"{question_number} {row['prompt']}\n"
    return questions_text

def doc_to_text(file):
    extension = os.path.splitext(file.name)[1].lower()
    if extension == ".docx":
        loader = Docx2txtLoader(file.name)
    elif extension == ".pdf":
        loader = PyPDFLoader(file.name)
    else:
        return "error"
    pages = loader.load()
    return "\n".join(page.page_content for page in pages)

def is_job_ad(text_fragment: str, model: ChatOpenAI) -> bool:
    """Sprawdza, czy fragment tekstu pochodzi z og≈Çoszenia o pracƒô."""
    try:
        prompt = PromptTemplate.from_template(
            "Czy poni≈ºszy tekst to fragment og≈Çoszenia o pracƒô? Odpowiedz tylko TAK lub NIE.\n\nTekst: {text_to_check}"
        )
        chain = prompt | model | StrOutputParser()
        response = chain.invoke({"text_to_check": text_fragment})
        return "TAK" in response.upper()
    except Exception:
        # W przypadku b≈Çƒôdu API, zak≈Çadamy, ≈ºe to nie jest og≈Çoszenie, aby przerwaƒá przetwarzanie.
        return False

def _generate_report(result: pd.DataFrame, title: str, prefix: str, include_more: bool) -> str:
    """Tworzy dokument Word na podstawie wynik√≥w analizy."""
    doc = Document('template.docx')
    doc.add_heading(title, 0)
    doc.add_paragraph(f'Data wygenerowania: {datetime.now().strftime("%d.%m.%Y %H:%M")}')
    
    for _, row in result.iterrows():
        doc.add_heading(str(row['area']), 1)
        doc.add_paragraph(str(row['citation']), style='Intense Quote')
        for line in str(row['content']).split('\n'):
            if line.strip():
                doc.add_paragraph(line)
        
        if include_more and pd.notna(row['more']):
            for line in str(row['more']).split('\n'):
                if line.strip():
                    doc.add_paragraph(line)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename_prefix = f"{prefix}{timestamp}_"
    
    with tempfile.NamedTemporaryFile(delete=False, prefix=filename_prefix, suffix=".docx") as tmp:
        doc.save(tmp.name)
        return tmp.name

def create_short_report(result: pd.DataFrame) -> str:
    return _generate_report(
        result,
        title='Raport analizy og≈Çoszenia o pracƒô (wersja skr√≥cona)',
        prefix='KoREKtor_short_',
        include_more=False
    )

def create_report(result: pd.DataFrame) -> str:
    return _generate_report(
        result,
        title='Raport analizy og≈Çoszenia o pracƒô',
        prefix='KoREKtor_pelny_',
        include_more=True
    )

def analyze_job_ad(job_ad, file):
    try:
        if file:
            job_ad = doc_to_text(file)
            if job_ad == "error":
                return {"error": "Nieobs≈Çugiwany format pliku. U≈ºyj PDF lub DOCX."}, None, None
        
        if not job_ad or job_ad.strip() == "":
            return None, None, None
        
        model = ChatOpenAI(temperature=0, model="gpt-4o-mini")

        # Krok 2: Weryfikacja, czy tekst jest og≈Çoszeniem o pracƒô
        text_for_verification = job_ad[:1500]
        if not is_job_ad(text_for_verification, model):
            return {"error": "Przes≈Çany tekst lub plik nie wyglƒÖda na og≈Çoszenie o pracƒô."}, None, None

        # Krok 3: G≈Ç√≥wna analiza z u≈ºyciem LLM
        questions = prepare_questions(matryca_df)
        prompt_template = PromptTemplate.from_template(PROMPT_TEMPLATE_TEXT)
    
        chain = prompt_template | model | parser
        response = chain.invoke({"job_ad": job_ad, "questions": questions})
    
        # Krok 4: Przetwarzanie odpowiedzi i budowanie DataFrame
        rows = []
        for i, answer_obj in enumerate(response.answers):
            if answer_obj.answer in {"TAK", "NIE"}:
                answer = answer_obj.answer
                # Inwersja odpowiedzi dla pytania nr 10, zgodnie z logikƒÖ matrycy.
                if i == 9:
                    answer = "NIE" if answer == "TAK" else "TAK"
                    
                new_row = {
                    'area': matryca_df.area[i],
                    'answer': answer,
                    'citation': answer_obj.citation,
                    'content': matryca_df.true[i] if answer == 'TAK' else matryca_df.false[i],
                    'more': matryca_df.more[i]
                }
                rows.append(new_row)
        
        output_df = pd.DataFrame(rows)
    
        # Krok 5: Generowanie raport√≥w i wyniku JSON
        short_word_file_path = create_short_report(output_df)
        word_file_path = create_report(output_df)
        # Wynik JSON jest tworzony na podstawie przetworzonych danych i udostƒôpniany w interfejsie.
        # Struktura: lista obiekt√≥w, gdzie ka≈ºdy obiekt to jeden wiersz analizy.
        json_output = output_df.to_dict(orient="records")
        
        return json_output, word_file_path, short_word_file_path
    except Exception as e:
        # Zwracamy b≈ÇƒÖd w formacie JSON, aby wy≈õwietliƒá go w interfejsie
        return {"error": f"WystƒÖpi≈Ç wewnƒôtrzny b≈ÇƒÖd serwera: {e}"}, None, None

# Interfejs Gradio dla g≈Ç√≥wnej analizy
analysis_demo = gr.Interface(
    fn=analyze_job_ad,
    inputs=[
        gr.TextArea(label="Og≈Çoszenie (opcjonalnie)", placeholder="Wklej tekst og≈Çoszenia tutaj..."), 
        gr.File(label="Lub wybierz plik PDF/DOCX", file_types=[".pdf", ".docx"]),
    ],
    outputs=[
        gr.JSON(label="Wyniki analizy (JSON)"), 
        gr.File(label="Pobierz pe≈Çny raport Word"), 
        gr.File(label="Pobierz skr√≥cony raport Word"),
    ],
    title="KoREKtor ‚Äì analiza og≈Çoszenia",
    description="Przeanalizuj og≈Çoszenie o pracƒô pod kƒÖtem dostƒôpno≈õci dla os√≥b z niepe≈Çnosprawno≈õciami"
)

def ask_hr_assistant(question):
    """Funkcja do zadawania pyta≈Ñ asystentowi HR."""
    global hr_assistant, HR_ASSISTANT_AVAILABLE
    if not HR_ASSISTANT_AVAILABLE or hr_assistant is None:
        return "‚ö†Ô∏è Ekspert HR nie jest dostƒôpny. Sprawd≈∫ konfiguracjƒô modu≈Çu hr_assistant i klucz OPENAI_API_KEY."
    try:
        response = hr_assistant.ask(question)
        answer = f"ü§ñ **Ekspert HR:**\n\n{response['answer']}"
        if response.get('sources'):
            answer += f"\n\nüìö **≈πr√≥d≈Ça:**\n"
            for i, source in enumerate(response['sources'][:3], 1):  # Max 3 ≈∫r√≥d≈Ça
                # Usuniƒôcie nazwy pliku ze ≈∫r√≥d≈Ça
                answer += f"{i}. str. {source.get('page', '?')}\n"
        return answer
    except Exception as e:
        return f"‚ùå WystƒÖpi≈Ç b≈ÇƒÖd podczas komunikacji z ekspertem HR: {e}"

# Interfejs Gradio dla asystenta HR
hr_assistant_demo = gr.Interface(
    fn=ask_hr_assistant,
    inputs=gr.TextArea(label="Pytanie do eksperta HR", placeholder="Zadaj pytanie..."),
    outputs=gr.Markdown(label="Odpowied≈∫ eksperta HR"),
    title="KoREKtor ‚Äì Ekspert HR",
    description="Zadaj pytanie ekspertowi HR w zakresie zatrudniania os√≥b z niepe≈Çnosprawno≈õciami."
)

# ≈ÅƒÖczenie interfejs√≥w w zak≈Çadki
demo = gr.TabbedInterface([analysis_demo, hr_assistant_demo], ["Analiza Og≈Çoszenia", "Ekspert HR"])

demo.launch()